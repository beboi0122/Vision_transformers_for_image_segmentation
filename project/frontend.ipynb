{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12f94d30",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T23:16:23.997246Z",
     "start_time": "2024-11-04T23:16:18.460709Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.12\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Python3.12\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import kagglehub.config\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gradio as gr\n",
    "\n",
    "from RoadDataLoader import RoadDataLoader\n",
    "from RoadDataset import RoadDataset\n",
    "\n",
    "from baseline_models.DeepLabV3Model import DeepLabV3Model\n",
    "from baseline_models.UNET2D import UNET2D\n",
    "from Swin_UNET.swin_transformer_unet_skip_expand_decoder_sys import SwinTransformerSys\n",
    "\n",
    "from wrapper_modules.RoadSegmentationModule import RoadSegmentationModule\n",
    "\n",
    "from loss_and_eval_functions import dice_score, combined_loss, iou_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1da9ed",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead30fbba0f0c53d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T23:16:24.014841Z",
     "start_time": "2024-11-04T23:16:24.008265Z"
    }
   },
   "outputs": [],
   "source": [
    "# get kaggle credentials file from ./kaggle.json\n",
    "with open(\"./kaggle.json\", \"r\") as f:\n",
    "    kaggle_json = json.load(f)\n",
    "kaggel_username = kaggle_json[\"username\"]\n",
    "kaggel_key = kaggle_json[\"key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6075279f90aa321d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T23:16:24.084443Z",
     "start_time": "2024-11-04T23:16:24.080064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle credentials set.\n"
     ]
    }
   ],
   "source": [
    "kagglehub.config.set_kaggle_credentials(kaggel_username, kaggel_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6133440ec03dbfcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T23:19:26.278471Z",
     "start_time": "2024-11-04T23:16:24.210056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"payne18/road-detection-dataset-with-masks\")\n",
    "# make data folder if it does not exist\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.mkdir(\"./data\")\n",
    "# Move data folder to ./data\n",
    "os.system(f\"mv {path} ./data/road-detection-dataset-with-masks\")\n",
    "# remove empty folder\n",
    "folder_to_remove = path.split(\"payne18/road-detection-dataset-with-masks\")[0] \n",
    "os.system((f\"rm -r {folder_to_remove}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c405d6276110b46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T23:19:40.828038Z",
     "start_time": "2024-11-04T23:19:40.825143Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"./data/road-detection-dataset-with-masks/deepglobe-road-extraction-dataset\"\n",
    "metadata_path = \"./data/road-detection-dataset-with-masks/deepglobe-road-extraction-dataset/metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ca61d45fc394f71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T23:19:40.910862Z",
     "start_time": "2024-11-04T23:19:40.874177Z"
    }
   },
   "outputs": [],
   "source": [
    "# open metadata\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "metadata = metadata[metadata[\"split\"] == \"train\"]\n",
    "metadata[\"sat_image_path\"] = metadata[\"sat_image_path\"].apply(lambda x: os.path.join(data_path, x))\n",
    "metadata[\"mask_path\"] = metadata[\"mask_path\"].apply(lambda x: os.path.join(data_path, x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6749043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 8\n",
    "optimizer = \"Adam\"\n",
    "lr = 1e-3\n",
    "weight_decay = 0.01\n",
    "epochs = 40\n",
    "loss_fn = combined_loss\n",
    "accelerator = \"auto\"\n",
    "pretrained = False\n",
    "image_size = 512\n",
    "num_workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc5e586c21ea1bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T02:09:30.604993Z",
     "start_time": "2024-11-05T02:09:15.480720Z"
    }
   },
   "outputs": [],
   "source": [
    "deeplabv3_model_path = \"./models/DeepLabV3_best_model.cpkt\"\n",
    "unet2d_model_path = \"./models/UNET2D_best_model.cpkt\"\n",
    "swin_model_path = \"./models/Swin_UNET.cpkt\"\n",
    "\n",
    "if not os.path.exists(\"./models\"):\n",
    "    os.mkdir(\"./models\")\n",
    "#check if file exists\n",
    "if not os.path.exists(deeplabv3_model_path):\n",
    "    !curl -L -o ./models/DeepLabV3_best_model.cpkt https://huggingface.co/beboi0122/Vision_transformers_for_image_segmentation_HF/resolve/main/DeepLabV3_best_model.cpkt\n",
    "if not os.path.exists(unet2d_model_path):\n",
    "    !curl -L -o ./models/UNET2D_best_model.cpkt https://huggingface.co/beboi0122/Vision_transformers_for_image_segmentation_HF/resolve/main/UNET2D_best_model.cpkt\n",
    "if not os.path.exists(swin_model_path):\n",
    "    !curl -L -o ./models/Swin_UNET.cpkt https://huggingface.co/beboi0122/Vision_transformers_for_image_segmentation_HF/resolve/main/Swin_UNET.cpkt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0bb1eb",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4973c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e9eda6f2d8bb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T02:09:31.428294Z",
     "start_time": "2024-11-05T02:09:30.632363Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juras\\AppData\\Local\\Temp\\ipykernel_18908\\2657104931.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  deeplabv3_state_dict = torch.load(deeplabv3_model_path)[\"state_dict\"]\n",
      "c:\\Python3.12\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Python3.12\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "deeplabv3_state_dict = torch.load(deeplabv3_model_path)[\"state_dict\"]\n",
    "deeplabv3_state_dict = {k.replace(\"model.model.\", \"model.\"): v for k, v in deeplabv3_state_dict.items()}\n",
    "deeplabv3_model = DeepLabV3Model(pretrained=False)\n",
    "deeplabv3_model.load_state_dict(deeplabv3_state_dict, strict=False)\n",
    "deeplabv3_module = RoadSegmentationModule(deeplabv3_model, combined_loss, optimizer)\n",
    "deeplabv3_model.to(device)\n",
    "_ = deeplabv3_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a5295fd50ee87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T02:10:06.627036Z",
     "start_time": "2024-11-05T02:10:06.523362Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juras\\AppData\\Local\\Temp\\ipykernel_18908\\2633616307.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unet2d_state_dict = torch.load(unet2d_model_path)[\"state_dict\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNET2D(\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList(\n",
       "    (0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (4): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (6): ConvTranspose2d(16, 8, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (7): Sequential(\n",
       "      (0): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (bottleneck): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (final_block): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet2d_state_dict = torch.load(unet2d_model_path)[\"state_dict\"]\n",
    "unet2d_state_dict = {k.replace(\"model.\", \"\"): v for k, v in unet2d_state_dict.items()}\n",
    "unet2d_model = UNET2D(3, 1, chanel_list=[8, 16, 32, 64])\n",
    "unet2d_model.load_state_dict(unet2d_state_dict, strict=False)\n",
    "unet2d_model.to(device)\n",
    "_ = unet2d_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba896e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.12\\Lib\\site-packages\\torch\\functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3596.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformerSys expand initial----depths:(2, 2, 6, 2);depths_decoder:[1, 2, 2, 2];drop_path_rate:0.1;num_classes:1\n",
      "---final upsample expand_first---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juras\\AppData\\Local\\Temp\\ipykernel_18908\\1306158744.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  module.load_state_dict(torch.load(swin_model_path)[\"state_dict\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_model = SwinTransformerSys(\n",
    "    img_size=image_size,           # Input méret\n",
    "    patch_size=4,           # Patch méret\n",
    "    in_chans=3,             # RGB képek\n",
    "    num_classes=1,          # Bináris szegmentáció\n",
    "    embed_dim=8,\n",
    "    num_heads = (1, 2, 4, 8),\n",
    "    depths = (2, 2, 6, 2),\n",
    "    window_size=8,          # Ablak méret\n",
    "    mlp_ratio=4.0,          # MLP arány\n",
    "    qkv_bias=True,          # QKV bias\n",
    "    drop_rate=0.1,          # Dropout ráta\n",
    "    attn_drop_rate=0.1,     # Attention dropout\n",
    "    drop_path_rate=0.1,     # Drop path\n",
    "    norm_layer=nn.LayerNorm,# Rétegnormálás\n",
    "    ape=False,              # Absolute positional embedding\n",
    "    patch_norm=True,        # Patch normálás\n",
    "    use_checkpoint=False    # Checkpoint\n",
    ")\n",
    "_ = swin_model.to(device)\n",
    "module = RoadSegmentationModule(swin_model, combined_loss, optimizer)\n",
    "module.load_state_dict(torch.load(swin_model_path)[\"state_dict\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f947d65",
   "metadata": {},
   "source": [
    "# Start Frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dcb1bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = RoadDataset(metadata=metadata, train=False)\n",
    "max_images = metadata.shape[0]\n",
    "\n",
    "#image evaluation based on model\n",
    "def predict_image(input_int, model):\n",
    "    img, mask = dataSet.__getitem__(input_int-1)\n",
    "    print(img.shape)\n",
    "    if model == \"UNet2D\":\n",
    "        with torch.no_grad():\n",
    "            output = unet2d_model(img.unsqueeze(0).to(device))\n",
    "            iou = iou_score(output.to(device),mask.to(device)).cpu()\n",
    "            pred = torch.sigmoid(output).cpu()\n",
    "    elif model == \"DeeplabV3\":\n",
    "        with torch.no_grad():\n",
    "            output = deeplabv3_model(img.unsqueeze(0).to(device))\n",
    "            print('bbbb')\n",
    "            iou = iou_score(output.to(device),mask.to(device)).cpu()\n",
    "            pred = torch.sigmoid(output).cpu()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            output = swin_model(img.unsqueeze(0).to(device))\n",
    "            iou = iou_score(output.to(device),mask.to(device)).cpu()\n",
    "            pred = torch.sigmoid(output).cpu()\n",
    "        \n",
    "    pred = pred.squeeze(0)\n",
    "    pred = pred.squeeze(0)\n",
    "    pred_numpy = pred.numpy()\n",
    "    mask = mask.cpu().squeeze(0).numpy()\n",
    "    \n",
    "    intersection = np.minimum(mask, pred_numpy)\n",
    "    union = np.maximum(mask, pred_numpy)\n",
    "    error = union - intersection\n",
    "    return (img*0.5+0.5).cpu().permute(1,2,0).numpy(), mask, pred_numpy, intersection, union, error, iou.item()*100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2387988c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\gradio\\queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\gradio\\route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\gradio\\blocks.py\", line 2043, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\gradio\\blocks.py\", line 1590, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\gradio\\utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\juras\\AppData\\Local\\Temp\\ipykernel_18908\\576659636.py\", line 15, in predict_image\n",
      "    output = deeplabv3_model(img.unsqueeze(0).to(device))\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\OneDrive\\Egyetem\\MSc\\2.Felev\\Melytanulas\\HF\\Vision_transformers_for_image_segmentation\\project\\baseline_models\\DeepLabV3Model.py\", line 26, in forward\n",
      "    return self.model(x)[\"out\"]\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torchvision\\models\\segmentation\\_utils.py\", line 27, in forward\n",
      "    x = self.classifier(x)\n",
      "        ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torchvision\\models\\segmentation\\deeplabv3.py\", line 111, in forward\n",
      "    _res.append(conv(x))\n",
      "                ^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torchvision\\models\\segmentation\\deeplabv3.py\", line 81, in forward\n",
      "    x = mod(x)\n",
      "        ^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\", line 193, in forward\n",
      "    return F.batch_norm(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torch\\nn\\functional.py\", line 2810, in batch_norm\n",
      "    _verify_batch_size(input.size())\n",
      "  File \"c:\\Python3.12\\Lib\\site-packages\\torch\\nn\\functional.py\", line 2776, in _verify_batch_size\n",
      "    raise ValueError(\n",
      "ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 256, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Gradio frontend\n",
    "with gr.Blocks() as demo:\n",
    "# Inputs\n",
    "    with gr.Row():\n",
    "        slider = gr.Slider(label=\"Select an Integer\", minimum=1, maximum=max_images, step=1)\n",
    "        method = gr.Radio(\n",
    "            choices=[\"UNet2D\", \"DeeplabV3\", \"Swin-Unet\"],\n",
    "            value=\"UNet2D\",\n",
    "            label=\"Select the method\"\n",
    "        )\n",
    "        exec = gr.Button(\"See\")\n",
    "# Outputs\n",
    "    with gr.Row():\n",
    "        img = gr.Image(type=\"pil\", label=\"Original\")\n",
    "        img2 = gr.Image(type=\"pil\", label=\"Mask\")\n",
    "        img3 = gr.Image(type=\"pil\", label=\"Prediction\")\n",
    "\n",
    "    with gr.Row():\n",
    "        intersection = gr.Image(type=\"pil\", label=\"Intersection\")\n",
    "        union =  gr.Image(type=\"pil\", label=\"Union\")\n",
    "        error =  gr.Image(type=\"pil\", label=\"Error\")\n",
    "    iou = gr.Textbox(label=\"IoU Percentage\")\n",
    "    exec.click(fn=predict_image, inputs=[slider, method], outputs=[img, img2, img3, intersection, union, error,  iou])\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
