{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12f94d30",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T23:16:23.997246Z",
     "start_time": "2024-11-04T23:16:18.460709Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import kagglehub.config\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gradio as gr\n",
    "\n",
    "from RoadDataLoader import RoadDataLoader\n",
    "from RoadDataset import RoadDataset\n",
    "\n",
    "from baseline_models.DeepLabV3Model import DeepLabV3Model\n",
    "from baseline_models.UNET2D import UNET2D\n",
    "from Swin_UNET.swin_transformer_unet_skip_expand_decoder_sys import SwinTransformerSys\n",
    "\n",
    "from wrapper_modules.RoadSegmentationModule import RoadSegmentationModule\n",
    "\n",
    "from loss_and_eval_functions import dice_score, combined_loss, iou_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1da9ed",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ead30fbba0f0c53d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T23:16:24.014841Z",
     "start_time": "2024-11-04T23:16:24.008265Z"
    }
   },
   "outputs": [],
   "source": [
    "# get kaggle credentials file from ./kaggle.json\n",
    "with open(\"./kaggle.json\", \"r\") as f:\n",
    "    kaggle_json = json.load(f)\n",
    "kaggel_username = kaggle_json[\"username\"]\n",
    "kaggel_key = kaggle_json[\"key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6075279f90aa321d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T23:16:24.084443Z",
     "start_time": "2024-11-04T23:16:24.080064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle credentials set.\n"
     ]
    }
   ],
   "source": [
    "kagglehub.config.set_kaggle_credentials(kaggel_username, kaggel_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6133440ec03dbfcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T23:19:26.278471Z",
     "start_time": "2024-11-04T23:16:24.210056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"payne18/road-detection-dataset-with-masks\")\n",
    "# make data folder if it does not exist\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.mkdir(\"./data\")\n",
    "# Move data folder to ./data\n",
    "os.system(f\"mv {path} ./data/road-detection-dataset-with-masks\")\n",
    "# remove empty folder\n",
    "folder_to_remove = path.split(\"payne18/road-detection-dataset-with-masks\")[0] \n",
    "os.system((f\"rm -r {folder_to_remove}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c405d6276110b46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T23:19:40.828038Z",
     "start_time": "2024-11-04T23:19:40.825143Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"./data/road-detection-dataset-with-masks/deepglobe-road-extraction-dataset\"\n",
    "metadata_path = \"./data/road-detection-dataset-with-masks/deepglobe-road-extraction-dataset/metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2ca61d45fc394f71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T23:19:40.910862Z",
     "start_time": "2024-11-04T23:19:40.874177Z"
    }
   },
   "outputs": [],
   "source": [
    "# open metadata\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "metadata = metadata[metadata[\"split\"] == \"train\"]\n",
    "metadata[\"sat_image_path\"] = metadata[\"sat_image_path\"].apply(lambda x: os.path.join(data_path, x))\n",
    "metadata[\"mask_path\"] = metadata[\"mask_path\"].apply(lambda x: os.path.join(data_path, x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6749043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 8\n",
    "optimizer = \"Adam\"\n",
    "lr = 1e-3\n",
    "weight_decay = 0.01\n",
    "epochs = 40\n",
    "loss_fn = combined_loss\n",
    "accelerator = \"auto\"\n",
    "pretrained = False\n",
    "image_size = 512\n",
    "num_workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a48ff6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>split</th>\n",
       "      <th>sat_image_path</th>\n",
       "      <th>mask_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2988</th>\n",
       "      <td>524867</td>\n",
       "      <td>train</td>\n",
       "      <td>./data/road-detection-dataset-with-masks/deepg...</td>\n",
       "      <td>./data/road-detection-dataset-with-masks/deepg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>110680</td>\n",
       "      <td>train</td>\n",
       "      <td>./data/road-detection-dataset-with-masks/deepg...</td>\n",
       "      <td>./data/road-detection-dataset-with-masks/deepg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5370</th>\n",
       "      <td>869652</td>\n",
       "      <td>train</td>\n",
       "      <td>./data/road-detection-dataset-with-masks/deepg...</td>\n",
       "      <td>./data/road-detection-dataset-with-masks/deepg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4394</th>\n",
       "      <td>73033</td>\n",
       "      <td>train</td>\n",
       "      <td>./data/road-detection-dataset-with-masks/deepg...</td>\n",
       "      <td>./data/road-detection-dataset-with-masks/deepg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>40682</td>\n",
       "      <td>train</td>\n",
       "      <td>./data/road-detection-dataset-with-masks/deepg...</td>\n",
       "      <td>./data/road-detection-dataset-with-masks/deepg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id  split                                     sat_image_path  \\\n",
       "2988    524867  train  ./data/road-detection-dataset-with-masks/deepg...   \n",
       "72      110680  train  ./data/road-detection-dataset-with-masks/deepg...   \n",
       "5370    869652  train  ./data/road-detection-dataset-with-masks/deepg...   \n",
       "4394     73033  train  ./data/road-detection-dataset-with-masks/deepg...   \n",
       "2152     40682  train  ./data/road-detection-dataset-with-masks/deepg...   \n",
       "\n",
       "                                              mask_path  \n",
       "2988  ./data/road-detection-dataset-with-masks/deepg...  \n",
       "72    ./data/road-detection-dataset-with-masks/deepg...  \n",
       "5370  ./data/road-detection-dataset-with-masks/deepg...  \n",
       "4394  ./data/road-detection-dataset-with-masks/deepg...  \n",
       "2152  ./data/road-detection-dataset-with-masks/deepg...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataLoader = RoadDataLoader(metadata, batch_size=batch_size, image_size=image_size, num_workers=num_workers)\n",
    "# dataLoader.setup()\n",
    "dataLoader.train_data = pd.read_csv(\"./metadata/traindata.csv\", index_col=0)\n",
    "dataLoader.val_data = pd.read_csv(\"./metadata/valdata.csv\", index_col=0)\n",
    "dataLoader.test_data = pd.read_csv(\"./metadata/testdata.csv\", index_col=0)\n",
    "dataLoader.train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bc5e586c21ea1bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T02:09:30.604993Z",
     "start_time": "2024-11-05T02:09:15.480720Z"
    }
   },
   "outputs": [],
   "source": [
    "deeplabv3_model_path = \"./models/DeepLabV3_best_model.cpkt\"\n",
    "unet2d_model_path = \"./models/UNET2D_best_model.cpkt\"\n",
    "swin_model_path = \"./models/Swin_UNET.cpkt\"\n",
    "\n",
    "if not os.path.exists(\"./models\"):\n",
    "    os.mkdir(\"./models\")\n",
    "#check if file exists\n",
    "if not os.path.exists(deeplabv3_model_path):\n",
    "    !curl -L -o ./models/DeepLabV3_best_model.cpkt https://huggingface.co/beboi0122/Vision_transformers_for_image_segmentation_HF/resolve/main/DeepLabV3_best_model.cpkt\n",
    "if not os.path.exists(unet2d_model_path):\n",
    "    !curl -L -o ./models/UNET2D_best_model.cpkt https://huggingface.co/beboi0122/Vision_transformers_for_image_segmentation_HF/resolve/main/UNET2D_best_model.cpkt\n",
    "if not os.path.exists(swin_model_path):\n",
    "    !curl -L -o ./models/Swin_UNET.cpkt https://huggingface.co/beboi0122/Vision_transformers_for_image_segmentation_HF/resolve/main/Swin_UNET.cpkt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0bb1eb",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c4973c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f34e9eda6f2d8bb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T02:09:31.428294Z",
     "start_time": "2024-11-05T02:09:30.632363Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juras\\AppData\\Local\\Temp\\ipykernel_16976\\257650887.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  deeplabv3_state_dict = torch.load(deeplabv3_model_path)[\"state_dict\"]\n",
      "c:\\Python3.12\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Python3.12\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "deeplabv3_state_dict = torch.load(deeplabv3_model_path)[\"state_dict\"]\n",
    "deeplabv3_state_dict = {k.replace(\"model.model.\", \"model.\"): v for k, v in deeplabv3_state_dict.items()}\n",
    "deeplabv3_model = DeepLabV3Model(pretrained=False)\n",
    "deeplabv3_model.load_state_dict(deeplabv3_state_dict, strict=False)\n",
    "_ = deeplabv3_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "419a5295fd50ee87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T02:10:06.627036Z",
     "start_time": "2024-11-05T02:10:06.523362Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juras\\AppData\\Local\\Temp\\ipykernel_16976\\4001304738.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unet2d_state_dict = torch.load(unet2d_model_path)[\"state_dict\"]\n"
     ]
    }
   ],
   "source": [
    "unet2d_state_dict = torch.load(unet2d_model_path)[\"state_dict\"]\n",
    "unet2d_state_dict = {k.replace(\"model.\", \"\"): v for k, v in unet2d_state_dict.items()}\n",
    "unet2d_model = UNET2D(3, 1, chanel_list=[8, 16, 32, 64])\n",
    "unet2d_model.load_state_dict(unet2d_state_dict, strict=False)\n",
    "_ = unet2d_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba896e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformerSys expand initial----depths:(2, 2, 6, 2);depths_decoder:[1, 2, 2, 2];drop_path_rate:0.1;num_classes:1\n",
      "---final upsample expand_first---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juras\\AppData\\Local\\Temp\\ipykernel_16976\\1306158744.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  module.load_state_dict(torch.load(swin_model_path)[\"state_dict\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_model = SwinTransformerSys(\n",
    "    img_size=image_size,           # Input méret\n",
    "    patch_size=4,           # Patch méret\n",
    "    in_chans=3,             # RGB képek\n",
    "    num_classes=1,          # Bináris szegmentáció\n",
    "    embed_dim=8,\n",
    "    num_heads = (1, 2, 4, 8),\n",
    "    depths = (2, 2, 6, 2),\n",
    "    window_size=8,          # Ablak méret\n",
    "    mlp_ratio=4.0,          # MLP arány\n",
    "    qkv_bias=True,          # QKV bias\n",
    "    drop_rate=0.1,          # Dropout ráta\n",
    "    attn_drop_rate=0.1,     # Attention dropout\n",
    "    drop_path_rate=0.1,     # Drop path\n",
    "    norm_layer=nn.LayerNorm,# Rétegnormálás\n",
    "    ape=False,              # Absolute positional embedding\n",
    "    patch_norm=True,        # Patch normálás\n",
    "    use_checkpoint=False    # Checkpoint\n",
    ")\n",
    "_ = swin_model.to(device)\n",
    "module = RoadSegmentationModule(swin_model, combined_loss, optimizer)\n",
    "module.load_state_dict(torch.load(swin_model_path)[\"state_dict\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f947d65",
   "metadata": {},
   "source": [
    "# Start Frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb1bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = RoadDataset(metadata=metadata, train=False)\n",
    "max_images = metadata.shape[0]\n",
    "\n",
    "#image evaluation based on model\n",
    "def predict_image(input_int, model):\n",
    "    img, mask = dataSet.__getitem__(input_int-1)\n",
    "    if model == \"UNet2D\":\n",
    "        with torch.no_grad():\n",
    "            output = unet2d_model(img.unsqueeze(0).to(device))\n",
    "            iou = iou_score(output.to(device),mask.to(device)).cpu()\n",
    "            pred = torch.sigmoid(output).cpu()\n",
    "    elif model == \"DeeplabV3\":\n",
    "        with torch.no_grad():\n",
    "            output = deeplabv3_model(img.unsqueeze(0).to(device))\n",
    "            iou = iou_score(output.to(device),mask.to(device)).cpu()\n",
    "            pred = torch.sigmoid(output).cpu()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            output = swin_model(img.unsqueeze(0).to(device))\n",
    "            iou = iou_score(output.to(device),mask.to(device)).cpu()\n",
    "            pred = torch.sigmoid(output).cpu()\n",
    "        \n",
    "    pred = pred.squeeze(0)\n",
    "    pred = pred.squeeze(0)\n",
    "    pred_numpy = pred.numpy()\n",
    "    mask = mask.cpu().squeeze(0).numpy()\n",
    "    \n",
    "    intersection = np.minimum(mask, pred_numpy)\n",
    "    union = np.maximum(mask, pred_numpy)\n",
    "    error = union - intersection\n",
    "    return (img*0.5+0.5).cpu().permute(1,2,0).numpy(), mask, pred_numpy, intersection, union, error, iou.item()*100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2387988c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio frontend\n",
    "with gr.Blocks() as demo:\n",
    "# Inputs\n",
    "    with gr.Row():\n",
    "        slider = gr.Slider(label=\"Select an Integer\", minimum=1, maximum=max_images, step=1)\n",
    "        method = gr.Radio(\n",
    "            choices=[\"UNet2D\", \"DeeplabV3\", \"Swin-Unet\"],\n",
    "            value=\"UNet2D\",\n",
    "            label=\"Select the method\"\n",
    "        )\n",
    "        exec = gr.Button(\"See\")\n",
    "# Outputs\n",
    "    with gr.Row():\n",
    "        img = gr.Image(type=\"pil\", label=\"Original\")\n",
    "        img2 = gr.Image(type=\"pil\", label=\"Mask\")\n",
    "        img3 = gr.Image(type=\"pil\", label=\"Prediction\")\n",
    "\n",
    "    with gr.Row():\n",
    "        intersection = gr.Image(type=\"pil\", label=\"Intersection\")\n",
    "        union =  gr.Image(type=\"pil\", label=\"Union\")\n",
    "        error =  gr.Image(type=\"pil\", label=\"Error\")\n",
    "    iou = gr.Textbox(label=\"IoU Percentage\")\n",
    "    exec.click(fn=predict_image, inputs=[slider, method], outputs=[img, img2, img3, intersection, union, error,  iou])\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
